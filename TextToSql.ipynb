{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45feffac",
   "metadata": {},
   "source": [
    "# Decoder Only Transformer\n",
    "\n",
    "- 18 Million trainable Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09a8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import numpy as np\n",
    "from keras import mixed_precision\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "769b0161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "else:\n",
    "    print(\"Bruh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "542714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "dataset = load_dataset(\"spider\")\n",
    "training = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9beffdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schemas\n",
    "\n",
    "with open(\"tables.json\", \"r\") as f:\n",
    "    tables_json = json.load(f)\n",
    "\n",
    "schemas = {}\n",
    "for db in tables_json:\n",
    "    db_id = db[\"db_id\"]\n",
    "    schemas[db_id] = {\n",
    "        \"tables\": db[\"table_names_original\"],  \n",
    "        \"columns\": db[\"column_names_original\"], \n",
    "        \"column_types\": db[\"column_types\"],     \n",
    "        \"foreign_keys\": db[\"foreign_keys\"],     \n",
    "        \"primary_keys\": db[\"primary_keys\"],     \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ec7476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDatabaseSchema(schema):\n",
    "  tables = schema[\"tables\"]\n",
    "  columns = schema[\"columns\"]\n",
    "  col_types = schema[\"column_types\"]\n",
    "  schema_str = []\n",
    "  for i, table in enumerate(tables):\n",
    "      table_cols = [c[1] for c in columns if c[0] == i] \n",
    "      table_types = [col_types[j] for j, c in enumerate(columns) if c[0] == i]\n",
    "      schema_str.append(f\"Table: {table}\")\n",
    "      for col_name, col_type in zip(table_cols, table_types):\n",
    "          schema_str.append(f\"  - {col_name} ({col_type})\")\n",
    "  return \"\\n\".join(schema_str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a018bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many heads of the departments are older than 56 ? \n",
      " Table: department\n",
      "  - Department_ID (number)\n",
      "  - Name (text)\n",
      "  - Creation (text)\n",
      "  - Ranking (number)\n",
      "  - Budget_in_Billions (number)\n",
      "  - Num_Employees (number)\n",
      "Table: head\n",
      "  - head_ID (number)\n",
      "  - name (text)\n",
      "  - born_state (text)\n",
      "  - age (number)\n",
      "Table: management\n",
      "  - department_ID (number)\n",
      "  - head_ID (number)\n",
      "  - temporary_acting (text)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for example in training :\n",
    "  db_id = example[\"db_id\"]\n",
    "  schemaText = formatDatabaseSchema(schemas[db_id])\n",
    "  question = example[\"question\"]\n",
    "  sqlQuery = example[\"query\"]\n",
    " \n",
    "  input_text.append(f\"Question: {question} \\n {schemaText}\")\n",
    "  target_text.append(sqlQuery)\n",
    "\n",
    "print(input_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6dc829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9765\n",
      "1120\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "def sqlTokenization(query):\n",
    "  tokens = re.findall(r\"[A-Za-z_][A-Za-z0-9_]*|\\d+|[><=!]+|[\\(\\),;\\*]\", query)\n",
    "  return tokens\n",
    "\n",
    "sql_tokenized_seq = [\" \".join(sqlTokenization(q)) for q in target_text]\n",
    "tokenizer = Tokenizer(num_words= 10000, oov_token=\"<OOV>\", char_level=False,lower=False,filters=\"\")\n",
    "\n",
    "texts = [\n",
    "    f\"<Start> {inp} <Sep> {sql} <End>\"\n",
    "    for inp, sql in zip(input_text, sql_tokenized_seq)\n",
    "]\n",
    "\n",
    "tokenizer.fit_on_texts(texts) # train on both at same time because Decoder only model\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(input_text + sql_tokenized_seq)\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "print(max_seq_len)\n",
    "sequences = pad_sequences(sequences,max_seq_len,padding=\"post\",truncating=\"post\") \n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf6662ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_only_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 1119)]       0           []                               \n",
      "                                                                                                  \n",
      " Embedding_layer (Embedding)    (None, 1119, 256)    2500096     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 1119, 256)   0           ['Embedding_layer[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " self_attemtion_0 (MultiHeadAtt  (None, 1119, 256)   263168      ['tf.__operators__.add_35[0][0]',\n",
      " ention)                                                          'tf.__operators__.add_35[0][0]',\n",
      "                                                                  'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 1119, 256)   0           ['self_attemtion_0[0][0]',       \n",
      " ambda)                                                           'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_36[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feed_forward_0 (Sequential)    (None, 1119, 256)    525568      ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 1119, 256)   0           ['feed_forward_0[0][0]',         \n",
      " ambda)                                                           'layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_37[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " self_attemtion_1 (MultiHeadAtt  (None, 1119, 256)   263168      ['layer_normalization_33[0][0]', \n",
      " ention)                                                          'layer_normalization_33[0][0]', \n",
      "                                                                  'layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 1119, 256)   0           ['self_attemtion_1[0][0]',       \n",
      " ambda)                                                           'layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_38[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feed_forward_1 (Sequential)    (None, 1119, 256)    525568      ['layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 1119, 256)   0           ['feed_forward_1[0][0]',         \n",
      " ambda)                                                           'layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_39[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " self_attemtion_2 (MultiHeadAtt  (None, 1119, 256)   263168      ['layer_normalization_35[0][0]', \n",
      " ention)                                                          'layer_normalization_35[0][0]', \n",
      "                                                                  'layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_40 (TFOpL  (None, 1119, 256)   0           ['self_attemtion_2[0][0]',       \n",
      " ambda)                                                           'layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_40[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feed_forward_2 (Sequential)    (None, 1119, 256)    525568      ['layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_41 (TFOpL  (None, 1119, 256)   0           ['feed_forward_2[0][0]',         \n",
      " ambda)                                                           'layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_41[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " self_attemtion_3 (MultiHeadAtt  (None, 1119, 256)   263168      ['layer_normalization_37[0][0]', \n",
      " ention)                                                          'layer_normalization_37[0][0]', \n",
      "                                                                  'layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_42 (TFOpL  (None, 1119, 256)   0           ['self_attemtion_3[0][0]',       \n",
      " ambda)                                                           'layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_42[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feed_forward_3 (Sequential)    (None, 1119, 256)    525568      ['layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_43 (TFOpL  (None, 1119, 256)   0           ['feed_forward_3[0][0]',         \n",
      " ambda)                                                           'layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 1119, 256)   512         ['tf.__operators__.add_43[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_output_dense (Dense)   (None, 1119, 9766)   2509862     ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,168,998\n",
      "Trainable params: 8,168,998\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.9384\n",
      "Epoch 1: val_loss improved from inf to 0.22857, saving model to TextToSQL.keras\n",
      "2800/2800 [==============================] - 474s 168ms/step - loss: 0.5353 - accuracy: 0.9384 - val_loss: 0.2286 - val_accuracy: 0.9829 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Trainings settings\n",
    "embed_dim = 256\n",
    "num_heads = 8 \n",
    "ff_dim = 1024\n",
    "num_layers = 4\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len-1))\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True, name=\"Embedding_layer\")(inputs)\n",
    "positional_encoding = layers.Embedding(input_dim=max_seq_len,output_dim=embed_dim, name=\"postional_encoding_layer\")(tf.range(start=0, limit=max_seq_len-1,delta=1))\n",
    "\n",
    "x = embedding_layer + positional_encoding\n",
    "\n",
    "for i in range(num_layers):\n",
    "  selfAttention = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim//num_heads,dropout=0.1,name=f\"self_attemtion_{i}\")(query=x,value=x,key=x, use_causal_mask=True)\n",
    "\n",
    "  x1 = layers.LayerNormalization(epsilon=1e-6)(selfAttention + x)\n",
    "\n",
    "  ffn = keras.Sequential([\n",
    "    layers.Dense(ff_dim,activation=\"gelu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(embed_dim)\n",
    "  ], name=f\"feed_forward_{i}\")\n",
    "\n",
    "  ffn_output = ffn(x1)\n",
    "\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(ffn_output + x1)\n",
    "\n",
    "outputs = layers.Dense(\n",
    "  vocab_size,\n",
    "  activation=\"softmax\",\n",
    "  name=\"decoder_output_dense\"\n",
    ")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_only_transformer\")\n",
    "\n",
    "initial_learning_rate = 1e-4\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='TextToSQL.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.LambdaCallback(\n",
    "        on_batch_end=lambda batch, logs: tf.clip_by_global_norm([v for v in model.trainable_variables], 1.0)\n",
    "    )\n",
    "]\n",
    "\n",
    "X = sequences[:, :-1]  \n",
    "y = sequences[:, 1:]   \n",
    "\n",
    "batch_size = 4\n",
    "validation_split = 0.2 \n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=1,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf9ef569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
